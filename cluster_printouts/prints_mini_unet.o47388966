[INFO] Module GCC/11.3.0 loaded.
[INFO] Module Python/3.10.4 loaded.
[INFO] Module CUDA/12.3.0 loaded.
[codecarbon INFO @ 18:53:52] [setup] RAM Tracking...
[codecarbon INFO @ 18:53:52] [setup] GPU Tracking...
[codecarbon INFO @ 18:53:52] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 18:53:52] [setup] CPU Tracking...
[codecarbon INFO @ 18:53:52] Tracking Intel CPU via RAPL interface
[codecarbon INFO @ 18:53:52] >>> Tracker's metadata:
[codecarbon INFO @ 18:53:52]   Platform system: Linux-4.18.0-553.8.1.el8_10.x86_64-x86_64-with-glibc2.28
[codecarbon INFO @ 18:53:52]   Python version: 3.10.4
[codecarbon INFO @ 18:53:52]   CodeCarbon version: 2.5.0
[codecarbon INFO @ 18:53:52]   Available RAM : 32.000 GB
[codecarbon INFO @ 18:53:52]   CPU count: 8
[codecarbon INFO @ 18:53:52]   CPU model: Intel(R) Xeon(R) Platinum 8468
[codecarbon INFO @ 18:53:52]   GPU count: 1
[codecarbon INFO @ 18:53:52]   GPU model: 1 x NVIDIA H100
[codecarbon INFO @ 18:53:55] Saving emissions data to file /w0/tmp/slurm_wk560263.47388966/tmpsdnwzmir/codecarbon/emissions.csv
COMET INFO: Experiment is live on comet.com https://www.comet.com/simon-onyx/ssqc-rsw-miniunet/fda4f63240cc48aabce6f9026cd747c0

COMET ERROR: Error logging git-related information
Fraction of images in datasets ['lab_rsw_segmentation_1'] containing no welding nugget, per dataset: [('lab_rsw_segmentation_1', 0.12666666666666668), ('lab_rsw_segmentation_2', 0.13445378151260504), ('lab_rsw_segmentation_3', 0.45), ('lab_rsw_segmentation_4', 0.3225806451612903), ('lab_rsw_segmentation_5', 0.05), ('lab_rsw_segmentation_8', 0.0), ('lab_rsw_segmentation_9', 0.08620689655172414), ('lab_rsw_segmentation_10', 0.22413793103448276), ('lab_rsw_segmentation_11', 0.0)]
Dataset groups: ['lab']
========================================================================
GPU: NVIDIA H100
GPU Memory Allocated: 382.00 MB
GPU Memory Cached: 382.00 MB
========================================================================
Model architecture visualization saved to ../models/miniunet_architecture.png
/home/wk560263/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
Model was trained up to epoch:
0
==============================
Using device: cuda
Batch 1 processed in 0.4385 seconds
Training loss: 0.6378370523452759
0.695037841796875
0.695037841796875
Batch 2 processed in 0.2849 seconds
Training loss: 0.6365129947662354
0.761199951171875
0.761199951171875
Batch 3 processed in 0.2852 seconds
Training loss: 0.6406767964363098
0.589874267578125
0.589874267578125
Batch 4 processed in 0.2869 seconds
Training loss: 0.6422688364982605
0.619293212890625
0.619293212890625
Batch 5 processed in 0.2849 seconds
Training loss: 0.6396403312683105
0.6678466796875
0.6678466796875
Batch 6 processed in 0.2853 seconds
Training loss: 0.6398754119873047
0.61834716796875
0.61834716796875
Batch 7 processed in 0.2851 seconds
Training loss: 0.6388349533081055
0.65576171875
0.65576171875
Batch 8 processed in 0.2849 seconds
Training loss: 0.6400597095489502
0.62744140625
0.62744140625
Batch 9 processed in 0.2849 seconds
Training loss: 0.6444686651229858
0.538360595703125
0.538360595703125
Batch 10 processed in 0.2852 seconds
Training loss: 0.640672504901886
0.621307373046875
0.621307373046875
Batch 11 processed in 0.2855 seconds
Training loss: 0.6360741853713989
0.7178955078125
0.7178955078125
Batch 12 processed in 0.2846 seconds
Training loss: 0.6388676166534424
0.681304931640625
0.681304931640625
Batch 13 processed in 0.2850 seconds
Training loss: 0.638148307800293
0.654327392578125
0.654327392578125
Batch 14 processed in 0.2851 seconds
Training loss: 0.6372133493423462
0.685089111328125
0.685089111328125
Batch 15 processed in 0.2853 seconds
Training loss: 0.6356750130653381
0.68975830078125
0.68975830078125
Batch 16 processed in 0.2850 seconds
Training loss: 0.6394733190536499
0.6385498046875
0.6385498046875
[codecarbon INFO @ 18:54:10] Energy consumed for RAM : 0.000050 kWh. RAM Power : 12.0 W
[codecarbon INFO @ 18:54:10] Energy consumed for all GPUs : 0.000900 kWh. Total GPU Power : 215.7270907613006 W
[codecarbon INFO @ 18:54:10] Energy consumed for all CPUs : 0.002184 kWh. Total CPU Power : 523.3643832801848 W
[codecarbon INFO @ 18:54:10] 0.003134 kWh of electricity used since the beginning.
Batch 17 processed in 0.2848 seconds
Training loss: 0.6382995843887329
0.61761474609375
0.61761474609375
Batch 18 processed in 0.2852 seconds
Training loss: 0.6358526349067688
0.657958984375
0.657958984375
Batch 19 processed in 0.2849 seconds
Training loss: 0.6359919905662537
0.650360107421875
0.650360107421875
Batch 20 processed in 0.2848 seconds
Training loss: 0.6368198394775391
0.669281005859375
0.669281005859375
Batch 21 processed in 0.2858 seconds
Training loss: 0.6410501599311829
0.591766357421875
0.591766357421875
Batch 22 processed in 0.2848 seconds
Training loss: 0.6360899209976196
0.64581298828125
0.64581298828125
Batch 23 processed in 0.2849 seconds
Training loss: 0.6357438564300537
0.67535400390625
0.67535400390625
Batch 24 processed in 0.2851 seconds
Training loss: 0.6392285227775574
0.565216064453125
0.565216064453125
Batch 25 processed in 0.2854 seconds
Training loss: 0.6358280181884766
0.66571044921875
0.66571044921875
Batch 26 processed in 0.2847 seconds
Training loss: 0.6374561786651611
0.62738037109375
0.62738037109375
Batch 27 processed in 0.2852 seconds
Training loss: 0.6350626945495605
0.65216064453125
0.65216064453125
Batch 28 processed in 0.2849 seconds
Training loss: 0.637506902217865
0.613037109375
0.613037109375
Batch 29 processed in 0.2843 seconds
Training loss: 0.6323562264442444
0.749603271484375
0.749603271484375
Batch 30 processed in 0.2851 seconds
Training loss: 0.6409727334976196
0.52960205078125
0.52960205078125
Batch 31 processed in 0.2855 seconds
Training loss: 0.6341495513916016
0.637451171875
0.637451171875
Batch 32 processed in 0.2848 seconds
Training loss: 0.6344632506370544
0.671142578125
0.671142578125
Batch 33 processed in 0.4126 seconds
Training loss: 0.6392061710357666
0.5530894994735718
0.5530894994735718
Epoch 1 Training Loss: 0.6380
Epoch 1 Training IoU: 0.6435
Epoch 1 Training Dice: 0.6435
Epoch 1 Training Accuracy: 0.9371
Model saved to ../models/MiniUNet.pt
Epoch 1 completed in 16.8862 seconds
Batch 1 processed in 0.2848 seconds
Training loss: 0.6305367946624756
0.695037841796875
0.695037841796875
Batch 2 processed in 0.2853 seconds
Training loss: 0.6290914416313171
0.761199951171875
0.761199951171875
Batch 3 processed in 0.2853 seconds
Training loss: 0.6337896585464478
0.591827392578125
0.591827392578125
Batch 4 processed in 0.2850 seconds
Training loss: 0.6357207298278809
0.61883544921875
0.61883544921875
Batch 5 processed in 0.2852 seconds
Training loss: 0.6327332258224487
0.6678466796875
0.6678466796875
Batch 6 processed in 0.2852 seconds
Training loss: 0.6330156922340393
0.616180419921875
0.616180419921875
Batch 7 processed in 0.2849 seconds
Training loss: 0.6318258047103882
0.655792236328125
0.655792236328125
Batch 8 processed in 0.2851 seconds
Training loss: 0.6332240104675293
0.62774658203125
0.62774658203125
Batch 9 processed in 0.2858 seconds
Training loss: 0.6384081840515137
0.536407470703125
0.536407470703125
Batch 10 processed in 0.2856 seconds
Training loss: 0.6340346336364746
0.622467041015625
0.622467041015625
Batch 11 processed in 0.2850 seconds
Training loss: 0.6288167238235474
0.717254638671875
0.717254638671875
Batch 12 processed in 0.2859 seconds
Training loss: 0.6320445537567139
0.680816650390625
0.680816650390625
[codecarbon INFO @ 18:54:25] Energy consumed for RAM : 0.000100 kWh. RAM Power : 12.0 W
[codecarbon INFO @ 18:54:25] Energy consumed for all GPUs : 0.002284 kWh. Total GPU Power : 333.11360245092936 W
[codecarbon INFO @ 18:54:25] Energy consumed for all CPUs : 0.004351 kWh. Total CPU Power : 521.5964153098141 W
[codecarbon INFO @ 18:54:25] 0.006735 kWh of electricity used since the beginning.
Batch 13 processed in 0.2843 seconds
Training loss: 0.6312761306762695
0.65582275390625
0.65582275390625
Batch 14 processed in 0.2852 seconds
Training loss: 0.63016676902771
0.685089111328125
0.685089111328125
Batch 15 processed in 0.2852 seconds
Training loss: 0.6285629272460938
0.689727783203125
0.689727783203125
Batch 16 processed in 0.2849 seconds
Training loss: 0.6329024434089661
0.638092041015625
0.638092041015625
Batch 17 processed in 0.2853 seconds
Training loss: 0.6314757466316223
0.61920166015625
0.61920166015625
Batch 18 processed in 0.2846 seconds
Training loss: 0.6288372278213501
0.65972900390625
0.65972900390625
Batch 19 processed in 0.2848 seconds
Training loss: 0.6291320323944092
0.64996337890625
0.64996337890625
Batch 20 processed in 0.2867 seconds
Training loss: 0.6300879716873169
0.670166015625
0.670166015625
Batch 21 processed in 0.2856 seconds
Training loss: 0.6347554922103882
0.5936279296875
0.5936279296875
Batch 22 processed in 0.2848 seconds
Training loss: 0.629418134689331
0.647186279296875
0.647186279296875
Batch 23 processed in 0.2853 seconds
Training loss: 0.6288321018218994
0.6767578125
0.6767578125
Batch 24 processed in 0.2849 seconds
Training loss: 0.632872998714447
0.56842041015625
0.56842041015625
Batch 25 processed in 0.2847 seconds
Training loss: 0.6291767358779907
0.66571044921875
0.66571044921875
Batch 26 processed in 0.2852 seconds
Training loss: 0.6308743953704834
0.63031005859375
0.63031005859375
Batch 27 processed in 0.2860 seconds
Training loss: 0.6283938884735107
0.65130615234375
0.65130615234375
Batch 28 processed in 0.2849 seconds
Training loss: 0.6309889554977417
0.614990234375
0.614990234375
Batch 29 processed in 0.2852 seconds
Training loss: 0.62528395652771
0.749603271484375
0.749603271484375
Batch 30 processed in 0.2860 seconds
Training loss: 0.6348931789398193
0.529449462890625
0.529449462890625
Batch 31 processed in 0.2848 seconds
Training loss: 0.6273672580718994
0.637786865234375
0.637786865234375
Batch 32 processed in 0.2851 seconds
Training loss: 0.6278942823410034
0.670074462890625
0.670074462890625
Batch 33 processed in 0.2447 seconds
Training loss: 0.6330801844596863
0.5517400503158569
0.5517400503158569
Epoch 2 Training Loss: 0.6312
Epoch 2 Training IoU: 0.6438
Epoch 2 Training Dice: 0.6438
Epoch 2 Training Accuracy: 0.9372
Model saved to ../models/MiniUNet.pt
Epoch 2 completed in 16.5095 seconds
Batch 1 processed in 0.2848 seconds
Training loss: 0.6234792470932007
0.695037841796875
0.695037841796875
Batch 2 processed in 0.2853 seconds
Training loss: 0.6219547986984253
0.761199951171875
0.761199951171875
Batch 3 processed in 0.2851 seconds
Training loss: 0.6272549629211426
0.58917236328125
0.58917236328125
Batch 4 processed in 0.2856 seconds
Training loss: 0.6293678283691406
0.62109375
0.62109375
Batch 5 processed in 0.2851 seconds
Training loss: 0.626103401184082
0.66864013671875
0.66864013671875
Batch 6 processed in 0.2853 seconds
Training loss: 0.6263364553451538
0.615997314453125
0.615997314453125
Batch 7 processed in 0.2848 seconds
Training loss: 0.625087320804596
0.657073974609375
0.657073974609375
Batch 8 processed in 0.2857 seconds
Training loss: 0.6266434192657471
0.627105712890625
0.627105712890625
Batch 9 processed in 0.2854 seconds
Training loss: 0.632598876953125
0.53485107421875
0.53485107421875
[codecarbon INFO @ 18:54:40] Energy consumed for RAM : 0.000150 kWh. RAM Power : 12.0 W
[codecarbon INFO @ 18:54:40] Energy consumed for all GPUs : 0.003681 kWh. Total GPU Power : 335.542118368196 W
[codecarbon INFO @ 18:54:40] Energy consumed for all CPUs : 0.006493 kWh. Total CPU Power : 514.0053452589693 W
[codecarbon INFO @ 18:54:40] 0.010324 kWh of electricity used since the beginning.
Batch 10 processed in 0.2850 seconds
Training loss: 0.6276930570602417
0.621673583984375
0.621673583984375
Batch 11 processed in 0.2851 seconds
Training loss: 0.6216813325881958
0.717132568359375
0.717132568359375
Batch 12 processed in 0.2859 seconds
Training loss: 0.6254318952560425
0.680511474609375
0.680511474609375
Batch 13 processed in 0.2850 seconds
Training loss: 0.6246154308319092
0.65478515625
0.65478515625
Batch 14 processed in 0.2863 seconds
Training loss: 0.6233619451522827
0.685089111328125
0.685089111328125
Batch 15 processed in 0.2852 seconds
Training loss: 0.6216060519218445
0.689727783203125
0.689727783203125
Batch 16 processed in 0.2849 seconds
Training loss: 0.6264240741729736
0.64007568359375
0.64007568359375
Batch 17 processed in 0.2864 seconds
Training loss: 0.6247999668121338
0.619415283203125
0.619415283203125
Batch 18 processed in 0.2857 seconds
Training loss: 0.621894121170044
0.659332275390625
0.659332275390625
Batch 19 processed in 0.2860 seconds
Training loss: 0.6223479509353638
0.650360107421875
0.650360107421875
Batch 20 processed in 0.2862 seconds
Training loss: 0.623416006565094
0.66998291015625
0.66998291015625
Batch 21 processed in 0.2858 seconds
Training loss: 0.6284307241439819
0.593109130859375
0.593109130859375
Batch 22 processed in 0.2849 seconds
Training loss: 0.6227465271949768
0.64422607421875
0.64422607421875
Batch 23 processed in 0.2852 seconds
Training loss: 0.621938943862915
0.67596435546875
0.67596435546875
Batch 24 processed in 0.2851 seconds
Training loss: 0.6264894008636475
0.564422607421875
0.564422607421875
Batch 25 processed in 0.2848 seconds
Training loss: 0.6223434805870056
0.66571044921875
0.66571044921875
Batch 26 processed in 0.2852 seconds
Training loss: 0.6242284178733826
0.632049560546875
0.632049560546875
Batch 27 processed in 0.2849 seconds
Training loss: 0.6214795112609863
0.65264892578125
0.65264892578125
Batch 28 processed in 0.2843 seconds
Training loss: 0.6244156360626221
0.6109619140625
0.6109619140625
Batch 29 processed in 0.2859 seconds
Training loss: 0.6179594993591309
0.749603271484375
0.749603271484375
Batch 30 processed in 0.2848 seconds
Training loss: 0.6286828517913818
0.52838134765625
0.52838134765625
Batch 31 processed in 0.2850 seconds
Training loss: 0.6203426718711853
0.637451171875
0.637451171875
Batch 32 processed in 0.2851 seconds
Training loss: 0.6208180785179138
0.671783447265625
0.671783447265625
Batch 33 processed in 0.2445 seconds
Training loss: 0.6267472505569458
0.5530184507369995
0.5530184507369995
Epoch 3 Training Loss: 0.6245
Epoch 3 Training IoU: 0.6436
Epoch 3 Training Dice: 0.6436
Epoch 3 Training Accuracy: 0.9371
Model saved to ../models/MiniUNet.pt
Epoch 3 completed in 16.5028 seconds
Batch 1 processed in 0.2855 seconds
Training loss: 0.6160769462585449
0.695037841796875
0.695037841796875
Model saved to ../models/MiniUNet.pt
Batch 2 processed in 0.2852 seconds
Training loss: 0.6142259240150452
0.761199951171875
0.761199951171875
Batch 3 processed in 0.2851 seconds
Training loss: 0.6200469732284546
0.58966064453125
0.58966064453125
Batch 4 processed in 0.2848 seconds
Training loss: 0.6225096583366394
0.617706298828125
0.617706298828125
Batch 5 processed in 0.2854 seconds
Training loss: 0.6188045740127563
0.6678466796875
0.6678466796875
Batch 6 processed in 0.2878 seconds
Training loss: 0.6191609501838684
0.61529541015625
0.61529541015625
[codecarbon INFO @ 18:54:55] Energy consumed for RAM : 0.000200 kWh. RAM Power : 12.0 W
[codecarbon INFO @ 18:54:55] Energy consumed for all GPUs : 0.005085 kWh. Total GPU Power : 336.92839718458765 W
[codecarbon INFO @ 18:54:55] Energy consumed for all CPUs : 0.008633 kWh. Total CPU Power : 513.6792602327639 W
[codecarbon INFO @ 18:54:55] 0.013918 kWh of electricity used since the beginning.
Batch 7 processed in 0.2848 seconds
Training loss: 0.6176765561103821
0.65814208984375
0.65814208984375
Batch 8 processed in 0.2860 seconds
Training loss: 0.6194005012512207
0.627105712890625
0.627105712890625
Batch 9 processed in 0.2849 seconds
Training loss: 0.6259058117866516
0.534332275390625
0.534332275390625
Batch 10 processed in 0.2848 seconds
Training loss: 0.6205487847328186
0.622161865234375
0.622161865234375
Batch 11 processed in 0.2851 seconds
Training loss: 0.6138682961463928
0.71893310546875
0.71893310546875
Batch 12 processed in 0.2849 seconds
Training loss: 0.6179646253585815
0.680389404296875
0.680389404296875
Batch 13 processed in 0.2849 seconds
Training loss: 0.6169743537902832
0.65533447265625
0.65533447265625
Batch 14 processed in 0.2851 seconds
Training loss: 0.6154866814613342
0.685089111328125
0.685089111328125
Batch 15 processed in 0.2848 seconds
Training loss: 0.6135529279708862
0.689727783203125
0.689727783203125
Batch 16 processed in 0.2853 seconds
Training loss: 0.6190066337585449
0.63812255859375
0.63812255859375
Batch 17 processed in 0.2854 seconds
Training loss: 0.6169450283050537
0.620086669921875
0.620086669921875
Batch 18 processed in 0.2855 seconds
Training loss: 0.6137203574180603
0.659149169921875
0.659149169921875
Batch 19 processed in 0.2848 seconds
Training loss: 0.6142969131469727
0.64996337890625
0.64996337890625
Batch 20 processed in 0.2852 seconds
Training loss: 0.6153608560562134
0.67156982421875
0.67156982421875
Batch 21 processed in 0.2849 seconds
Training loss: 0.6209748983383179
0.592254638671875
0.592254638671875
Batch 22 processed in 0.2852 seconds
Training loss: 0.6146722435951233
0.644927978515625
0.644927978515625
Batch 23 processed in 0.2855 seconds
Training loss: 0.6135372519493103
0.678070068359375
0.678070068359375
slurmstepd: error: *** JOB 47388966 ON n23g0021 CANCELLED AT 2024-07-22T18:55:04 ***
