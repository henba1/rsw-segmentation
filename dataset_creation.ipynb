{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def overlay_images(background_img, overlay_img, img_id, output_folder_overlay, opacity=0.15):\n",
    "    \"\"\"\n",
    "    Overlay one image with another using a specified opacity.\n",
    "    \"\"\"\n",
    "    # Load the images\n",
    "    if isinstance(background_img, str):\n",
    "        background_img = cv2.imread(background_img)\n",
    "    if isinstance(overlay_img, str):\n",
    "        overlay_img = cv2.imread(overlay_img)\n",
    "\n",
    "    overlayed_img = background_img.copy()\n",
    "    overlay = cv2.addWeighted(overlay_img, opacity, overlayed_img, 1 - opacity, 0, overlayed_img)\n",
    "    output_path = os.path.join(output_folder_overlay, f\"overlay_{img_id}.png\")\n",
    "    cv2.imwrite(output_path, overlay)\n",
    "    \n",
    "    return overlay\n",
    "\n",
    "def compute_masks(annotations_project_roots):\n",
    "    completed = True\n",
    "    not_completed = False\n",
    "\n",
    "    def is_completed(val):    \n",
    "        #gets welds flagged as completed, having no instances labelled, or having too few points to make a polygon in their labels\n",
    "        #points < 3*2 ?\n",
    "        b = (val['metadata']['status'] == \"Completed\" or (len(val['instances']) == 0 and val['metadata']['status'] == \"Not started\") or (len(val['instances']) == 1 and len(val['instances'][0]['points']) < 6))\n",
    "        return b\n",
    "\n",
    "    def is_not_completed(val):\n",
    "        #gets any welds not flagged as completed\n",
    "        return val['metadata']['status'] != \"Completed\"\n",
    "\n",
    "    for annotations_project_root in annotations_project_roots:\n",
    "        os.makedirs(os.path.join(annotations_project_root, \"masks\"), exist_ok=True)\n",
    "        output_directory = os.path.join(annotations_project_root, \"masks\")\n",
    "        with open(os.path.join(annotations_project_root, 'annotations_1.json')) as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        all_keys = [x for x in data.keys()]\n",
    "        all_keys = all_keys[1:]\n",
    "\n",
    "        if completed:\n",
    "            all_keys = [x for x in all_keys if is_completed(data[x])]\n",
    "        elif not_completed:\n",
    "            all_keys = [x for x in all_keys if is_not_completed(data[x])]\n",
    "        \n",
    "        for key in all_keys:\n",
    "            instances = data[key]['instances']\n",
    "            img = cv2.imread(os.path.join(annotations_project_root, 'images', str(key)))\n",
    "            mask_all = np.zeros_like(img[:,:,0], dtype=np.uint8)\n",
    "            if instances is None or len(instances) == 0:\n",
    "                instance_copy = np.zeros_like(img)\n",
    "                class_id = 1\n",
    "                background = np.ones_like(img, dtype=np.uint8) * 255\n",
    "                background_image_output_filename = os.path.join(output_directory, key.split('.')[0] + '-' + str('bg') + '.png')\n",
    "                cv2.imwrite(background_image_output_filename, background)\n",
    "            else:\n",
    "                background = np.ones_like(img, dtype=np.uint8) * 255\n",
    "                for instance in instances:\n",
    "                    instance_copy = np.zeros_like(img)\n",
    "\n",
    "                    class_id = instance['classId']\n",
    "                    points = instance['points']\n",
    "\n",
    "                    image_output_filename = os.path.join(output_directory, key.split('.')[0] + '-' + str(class_id) + '.png')\n",
    "\n",
    "                    if points:\n",
    "                        points = np.array(points).reshape((-1, 2))\n",
    "                        cv2.fillPoly(instance_copy, np.int32([points]), color=(255, 255, 255))\n",
    "                        background[background == 255] -= instance_copy[background == 255]\n",
    "                        cv2.fillPoly(mask_all, np.int32([points]), color=(class_id))\n",
    "                    else:\n",
    "                        cv2.imwrite(image_output_filename, instance_copy)\n",
    "                background_image_output_filename = os.path.join(output_directory, key.split('.')[0] + '-' + str('bg') + '.png')\n",
    "                cv2.imwrite(background_image_output_filename, background)\n",
    "            \n",
    "            #to get foreground mask, simply take the inverse of the background mask, so no need to do this separately\n",
    "\n",
    "\n",
    "\n",
    "def compute_image_info(project_root, error_log_file, pattern_img, pattern_poly, file_format):\n",
    "    all_images = []\n",
    "    token = False\n",
    "    \n",
    "    for project_root in project_root:\n",
    "        output_folder_overlay = f\"{project_root}/overlayed_labels\"\n",
    "        os.makedirs(output_folder_overlay, exist_ok=True)\n",
    "        img_list = []\n",
    "        with open(os.path.join(project_root, 'annotations_1.json')) as file:\n",
    "            data = json.load(file)\n",
    "        dataset_name = project_root.split(\"_\")[-4:]\n",
    "        dataset_name = \"_\".join(dataset_name)\n",
    "        dir_imgs = f\"{project_root}/images\"\n",
    "        dir_masks = f\"{project_root}/masks\"\n",
    "        directory_imgs = os.fsencode(dir_imgs)\n",
    "        directory_masks = os.fsencode(dir_masks)\n",
    "        list_dir_imgs = []\n",
    "\n",
    "        for x in os.listdir(directory_imgs):\n",
    "            #keep only files with image file format\n",
    "            filename_img = os.fsdecode(x)\n",
    "            if filename_img.lower().endswith(file_format):\n",
    "                list_dir_imgs.append(filename_img)\n",
    "\n",
    "        img_files = sorted(list_dir_imgs)\n",
    "        poly_files = sorted(os.listdir(directory_masks))\n",
    "        img_dict = {}\n",
    "        poly_dict = {} \n",
    "\n",
    "        for filename_img in img_files:\n",
    "            match = re.match(pattern_img, filename_img)\n",
    "            if match:\n",
    "                img_id = match.group(1)\n",
    "                img_dict[img_id] = filename_img\n",
    "        \n",
    "        for filename_poly in poly_files:\n",
    "            match = re.match(pattern_poly, os.fsdecode(filename_poly))\n",
    "            if match:\n",
    "                poly_id = match.group(1)\n",
    "                poly_dict[poly_id] = filename_poly.decode()\n",
    "\n",
    "        matched_pairs = [(img_dict[id], poly_dict[id]) for id in img_dict.keys() & poly_dict.keys()]\n",
    "        #1\n",
    "        with open(error_log_file, 'w') as error_file:\n",
    "            error_file.write(\"Unmatched Image IDs:\\n\")\n",
    "            error_file.write(\"\\n\".join(sorted(img_dict.keys() - poly_dict.keys())))\n",
    "            error_file.write(\"\\n\\nUnmatched Mask IDs:\\n\")\n",
    "            error_file.write(\"\\n\".join(sorted(poly_dict.keys() - img_dict.keys())))\n",
    "        #2.\n",
    "        for i, img_id in enumerate(img_dict.keys() & poly_dict.keys()):\n",
    "            filepath_img = str(os.path.join(directory_imgs.decode(), img_dict[img_id]))\n",
    "            filepath_poly = str(os.path.join(directory_masks.decode(), poly_dict[img_id]))\n",
    "            overlayed_img = overlay_images(filepath_img, filepath_poly, img_id, output_folder_overlay)\n",
    "            #overlayed_img = rgb_to_gray(overlayed_img)\n",
    "            #3.\n",
    "            filename_img = matched_pairs[i][0]\n",
    "            filename_poly = matched_pairs[i][1]\n",
    "            \n",
    "            try:\n",
    "                instance = data[filename_img]['instances']\n",
    "            except KeyError:\n",
    "                error_message = f'{dataset_name}: no annotation data for image {filename_img}'\n",
    "                #print(error_message)\n",
    "                error_file.write(error_message + '\\n')\n",
    "                instance = None\n",
    "                token = True\n",
    "            img = cv2.imread(filepath_img, cv2.IMREAD_GRAYSCALE)\n",
    "            #search in the annotation file 'annotation_1' for the image_id and get the meta information if token has not been set to True:\n",
    "            if not token:\n",
    "                label_status = data[filename_img]['metadata']['status']\n",
    "                if label_status == \"Not started\" or instance is None or len(instance) == 0:\n",
    "                    points = []\n",
    "                    classId = None\n",
    "                else:\n",
    "                    points = instance[0]['points']\n",
    "                    classId = instance[0]['classId']\n",
    "            else:\n",
    "                label_status = \"No annotation data available\"\n",
    "                points = []\n",
    "                classId = None\n",
    "                \n",
    "            token = False\n",
    "            img_list.append([img, overlayed_img, filename_img, filename_poly, dataset_name, points, classId, label_status])\n",
    "\n",
    "        all_images.append(img_list)\n",
    "\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_computed = True\n",
    "image_file_format = \".png\"\n",
    "base_path_lab = \"C:\\\\Users\\\\Hendrik\\\\iCloudDrive\\\\Desktop\\\\WZL\\\\ssqc\\\\rsw_research\\\\code_data\\\\data\\\\lab_rsw_segmentation_\"\n",
    "base_path_EUR = \"C:\\\\Users\\\\Hendrik\\\\iCloudDrive\\\\Desktop\\\\WZL\\\\ssqc\\\\rsw_research\\\\code_data\\\\data\\\\Ford_Tessonics_Europe_Lab_Data_\" \n",
    "lab_useable = [1,2,3,4,5,8,9,10,11]\n",
    "eur_useable = [1,4,5,6,7,10]\n",
    "annotations_project_roots_lab = [f'{base_path_lab}{x}' for x in lab_useable]\n",
    "annotations_project_roots_EUR = [f'{base_path_EUR}{x}' for x in eur_useable]\n",
    "\n",
    "pattern_img = r\"raw_image_(.*?)\\.png\"\n",
    "pattern_poly = r\"raw_image_(.*?)-bg\\.png\"\n",
    "\n",
    "data_sources = [\"eur\"] #[\"eur\", \"lab\"]\n",
    "#1.check if image and mask have the same ID\n",
    "#2.overlay image and mask to check if they fit (do visual inspection)\n",
    "#3.append image data with meta information for later differentiation\n",
    "#structure of entries : image, image_id/filename, dataset_of_origin, info from annotation file\n",
    "for source in data_sources:\n",
    "    if source == \"eur\":\n",
    "        if masks_computed == False:\n",
    "            compute_masks(annotations_project_roots_EUR)\n",
    "        error_log_file = os.path.join(f'non_matches_Europe_Lab_Data.txt')\n",
    "        all_images_data = compute_image_info(annotations_project_roots_EUR, error_log_file, pattern_img, pattern_poly, image_file_format)\n",
    "        np.save('../data/all_images_data_EUR.npy', np.array(all_images_data, dtype=object), allow_pickle=True)\n",
    "    else:\n",
    "        if masks_computed == False:\n",
    "            compute_masks(annotations_project_roots_lab)\n",
    "        error_log_file = os.path.join(f'non_matches_lab_rsw_segmentation.txt')\n",
    "        all_images_data = compute_image_info(annotations_project_roots_lab, error_log_file, pattern_img, pattern_poly, image_file_format)\n",
    "        np.save('../data/all_images_data_lab.npy', np.array(all_images_data, dtype=object), allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kizam_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
